# AmbonMUD Scalability Refactor Plan

## Context

AmbonMUD is currently a single-process MUD server. We want to evolve toward a horizontally-scaled gateway architecture ("scale the edge") with one authoritative game engine, async workers for heavy I/O, and Redis for caching/pub-sub â€” while keeping the single-process mode fully functional at every step.

**Target architecture:**
```
Clients (telnet/WS)                     Clients (telnet/WS)
       â”‚                                        â”‚
   Gateway A â”€â”€â”€â”€ gRPC bidi stream â”€â”€â”€â”€â”    Gateway B
       â”‚                               â”‚        â”‚
       â””â”€â”€â”€â”€â”€â”€ Redis pub/sub â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                               Engine Server
                            (single-threaded tick,
                             authoritative world)
                                       â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                         PersistenceWorker    Redis Cache
                              â”‚
                         YAML (durable)
```

**Choices made:**
- **gRPC** bidirectional streaming for gatewayâ†”engine
- **Incremental refactor** â€” 4 phases, each independently deployable
- **Async worker** starting with persistence only
- **Redis** for message bus (pub/sub) + cache; YAML stays as durable store

---

## Phase 1: Abstract the Event Transport Layer âœ… IMPLEMENTED

**Goal:** Extract `EventBus` interfaces to replace direct `Channel` usage, creating the seam for future gRPC swap. Also abstract session ID allocation.

### New files

| File | Purpose |
|------|---------|
| `src/main/kotlin/dev/ambon/bus/InboundBus.kt` | Interface: `suspend send()`, `trySend()`, `tryReceive()`, `close()` |
| `src/main/kotlin/dev/ambon/bus/OutboundBus.kt` | Interface: `suspend send()`, `tryReceive()`, `asReceiveChannel()`, `close()` |
| `src/main/kotlin/dev/ambon/bus/LocalInboundBus.kt` | Wraps `Channel<InboundEvent>` â€” preserves current behavior |
| `src/main/kotlin/dev/ambon/bus/LocalOutboundBus.kt` | Wraps `Channel<OutboundEvent>` â€” preserves current behavior |
| `src/main/kotlin/dev/ambon/session/SessionIdFactory.kt` | Interface: `fun allocate(): SessionId` |
| `src/main/kotlin/dev/ambon/session/AtomicSessionIdFactory.kt` | Current `AtomicLong` behavior, extracted from `MudServer` |

### Files to modify

| File | Change |
|------|--------|
| `MudServer.kt` | Replace `Channel<*>` with `Local*Bus`. Replace `sessionIdSeq` with `AtomicSessionIdFactory`. |
| `GameEngine.kt` | Params: `ReceiveChannel` â†’ `InboundBus`, `SendChannel` â†’ `OutboundBus` |
| `CommandRouter.kt` | `SendChannel<OutboundEvent>` â†’ `OutboundBus` |
| `CombatSystem.kt` | `SendChannel<OutboundEvent>` â†’ `OutboundBus` |
| `MobSystem.kt` | `SendChannel<OutboundEvent>` â†’ `OutboundBus` |
| `OutboundRouter.kt` | `ReceiveChannel<OutboundEvent>` â†’ `OutboundBus`, use `asReceiveChannel()` |
| `BlockingSocketTransport.kt` | `SendChannel<InboundEvent>` â†’ `InboundBus` |
| `KtorWebSocketTransport.kt` | Same as telnet |
| `NetworkSession.kt` | `SendChannel<InboundEvent>` â†’ `InboundBus` |
| All test files (~14) | Mechanical: `Channel(capacity)` â†’ `Local*Bus(capacity)` |

### Dependencies: None
### Config changes: None
### Risk: Low â€” pure interface extraction, zero behavioral change

### Verification
- `ktlintCheck test` passes
- All 31+ existing tests pass after mechanical constructor updates
- New unit tests for `LocalInboundBus`, `LocalOutboundBus`, `AtomicSessionIdFactory`

---

## Phase 2: Async Persistence Worker âœ… IMPLEMENTED

**Goal:** Move player saves off the engine tick into a background worker. Write-behind with dirty-flag coalescing.

### Current persistence call sites (all in `PlayerRegistry.kt`)
- `bindSession()` â€” on login
- `disconnect()` â†’ `persistIfClaimed()` â€” on logout
- `moveTo()` â†’ `persistIfClaimed()` â€” on every room change
- `grantXp()` â†’ `persistIfClaimed()` â€” on every kill
- `setAnsiEnabled()` â†’ `persistIfClaimed()` â€” on ANSI toggle

Reads (`findByName`, `findById`) must stay synchronous for login flow. Only writes become async.

### New files

| File | Purpose |
|------|---------|
| `persistence/WriteCoalescingPlayerRepository.kt` | Wraps `PlayerRepository`. `save()` â†’ marks dirty (no I/O). `find*()` â†’ checks in-memory cache first. Exposes `flushDirty()` and `flushAll()`. |
| `persistence/PersistenceWorker.kt` | Background coroutine: calls `flushDirty()` every N seconds on `Dispatchers.IO`. |

### Files to modify

| File | Change |
|------|--------|
| `MudServer.kt` | Wrap `YamlPlayerRepository` in `WriteCoalescingPlayerRepository`. Create and start `PersistenceWorker`. On `stop()`: flush before closing. |
| `AppConfig.kt` | Add `PersistenceWorkerConfig(flushIntervalMs: Long = 5000, enabled: Boolean = true)` |
| `application.yaml` | Add `persistence.worker.*` defaults |

### Key design: `PlayerRegistry` does NOT change â€” it still calls `repo.save()` and `repo.find*()`. The coalescing wrapper intercepts transparently.

### Shutdown sequence
1. Stop transports (no new connections)
2. `worker.shutdown()` â†’ `flushAll()` (write all dirty records)
3. Cancel engine
4. Close channels

### Dependencies: None
### Risk: Medium â€” crash between save and flush loses up to `flushIntervalMs` of data

### Verification
- `WriteCoalescingPlayerRepositoryTest` â€” verify cache, dirty tracking, coalescing (10 saves â†’ 1 write)
- `PersistenceWorkerTest` â€” verify periodic flush, shutdown flush
- Manual: connect, walk 10 rooms quickly, check YAML only written once
- `ktlintCheck test` passes

---

## Phase 3: Redis Integration âœ… IMPLEMENTED

**Goal:** Add Redis as cache + pub/sub bus. YAML stays durable. Redis is opt-in (`enabled: false` default).

> **Implementation notes (actual vs plan):**
> - `RedisCachingPlayerRepository` wraps the coalescing repo (not YAML directly as originally sketched)
> - Key scheme uses `player:id:<id>` instead of a hash (`player:<playerId>`) â€” JSON string, not Redis hash
> - Bus is split into `RedisInboundBus` / `RedisOutboundBus` (separate from a monolithic `RedisPubSubBus`)
> - `RedisSessionRegistry` was deferred to Phase 4 (not yet needed in single-engine mode)
> - `instanceId` is UUID auto-generated if not set in config

### New files

| File | Purpose |
|------|---------|
| `redis/RedisConnectionManager.kt` | Lettuce client lifecycle, typed connections |
| `persistence/RedisCachingPlayerRepository.kt` | Wraps coalescing repo. Write-through to Redis hash + dirty flag for YAML. Read: Redis â†’ in-memory cache â†’ YAML. |
| `bus/RedisPubSubBus.kt` | Publish/subscribe for server-wide broadcasts (gossip, shutdown). Injects received messages into local outbound bus. |
| `session/RedisSessionRegistry.kt` | Tracks `session:<id> â†’ {gatewayId, playerName}` in Redis. Prepares for Phase 4. |

### Redis data model
```
player:name:<lowercase>     â†’ playerId              # name index
player:<playerId>           â†’ {name, roomId, ...}   # full record hash
session:<sessionId>         â†’ {gatewayId, player}    # session ownership
channel:broadcast           â†’ pub/sub for gossip/shutdown
channel:gateway:<id>        â†’ targeted gateway messages
```

### Files to modify

| File | Change |
|------|--------|
| `AppConfig.kt` | Add `RedisConfig(enabled, host, port, password, database, cacheTtlSeconds)` |
| `application.yaml` | Add `ambonMUD.redis.*` defaults (disabled) |
| `MudServer.kt` | Conditionally create Redis components. Repository chain: YAML â†’ Redis cache â†’ write coalescing. Wire `RedisPubSubBus`. |
| `CommandRouter.kt` | Inject optional `broadcastBus` for gossip/shutdown events |

### New dependency
```kotlin
implementation("io.lettuce:lettuce-core:6.3.2.RELEASE")
```

### Risk: Medium â€” Redis is opt-in. Must handle connection failures gracefully (fall through to local).

### Verification
- Integration tests with embedded Redis (testcontainers or jedis-mock)
- `RedisCachingPlayerRepositoryTest`, `RedisPubSubBusTest`, `RedisSessionRegistryTest`
- Manual: start with `redis.enabled=true`, verify cache hits in logs
- Without Redis: identical to Phase 2

---

## Phase 4: gRPC Gateway Split ğŸ”² PLANNED

**Goal:** Split into Gateway (transports + routing) and Engine (game logic + persistence) processes, communicating via gRPC bidirectional streaming.

### New files

| File | Purpose |
|------|---------|
| `proto/ambonmud/v1/events.proto` | Protobuf messages mirroring `InboundEvent`/`OutboundEvent` |
| `proto/ambonmud/v1/engine_service.proto` | `service EngineService { rpc EventStream(stream Inbound) returns (stream Outbound); }` |
| `grpc/EngineGrpcServer.kt` | Hosts gRPC service, accepts gateway connections |
| `grpc/EngineServiceImpl.kt` | Bidirectional streaming: protoâ†”domain conversion, sessionâ†’stream routing |
| `grpc/ProtoMapper.kt` | Bidirectional `InboundEvent`/`OutboundEvent` â†” proto conversion |
| `bus/GrpcInboundBus.kt` | `InboundBus` impl that sends over gRPC stream |
| `bus/GrpcOutboundBus.kt` | `OutboundBus` impl that receives from gRPC stream |
| `gateway/GatewayMain.kt` | Entry point for gateway process |
| `gateway/GatewayConfig.kt` | Gateway-specific config |
| `session/SnowflakeSessionIdFactory.kt` | Globally unique IDs: `(gatewayId << 48) \| (timestamp << 16) \| seq` |

### Three deployment modes (`ambonMUD.mode`)

| Mode | Runs | gRPC | Redis |
|------|------|------|-------|
| `standalone` (default) | Everything in-process | No | Optional |
| `engine` | GameEngine + gRPC server + persistence | Server | Required |
| `gateway` | Transports + OutboundRouter + gRPC client | Client | Required |

### Files to modify

| File | Change |
|------|--------|
| `MudServer.kt` | Add `mode` switch: standalone/engine/gateway component selection |
| `Main.kt` | Route to appropriate startup based on mode |
| `AppConfig.kt` | Add `GrpcConfig`, `GatewayConfig`, `DeploymentMode` enum |
| `application.yaml` | Add `ambonMUD.mode`, `grpc.*`, `gateway.*` defaults |

### New dependencies
```kotlin
implementation("io.grpc:grpc-netty-shaded:1.62.2")
implementation("io.grpc:grpc-protobuf:1.62.2")
implementation("io.grpc:grpc-kotlin-stub:1.4.1")
implementation("com.google.protobuf:protobuf-kotlin:3.25.3")
plugins { id("com.google.protobuf") version "0.9.4" }
```

### Session routing
- Each `Connected` event arrives on a specific gRPC stream â†’ engine maps `sessionId â†’ stream`
- Outbound events routed to owning stream by sessionId lookup
- Redis `session:*` keys provide backup/recovery and cross-gateway operations

### Risk: High â€” largest change, introduces network failure modes. Mitigated by standalone mode always working.

### Verification
- `ProtoMapperTest` â€” round-trip every event variant
- `EngineServiceImplTest` â€” bidirectional streaming with in-process gRPC
- `SnowflakeSessionIdFactoryTest` â€” uniqueness across gateway IDs
- Integration: engine + gateway in-JVM, run connect â†’ login â†’ say â†’ quit flow
- `ktlintCheck test` passes

---

## Phase Dependency Graph

```
Phase 1 (EventBus)
    â”‚
    â”œâ”€â”€â†’ Phase 2 (Async persistence)
    â”‚        â”‚
    â”‚        â””â”€â”€â†’ Phase 3 (Redis)
    â”‚                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â†’ Phase 4 (gRPC split)
```

Each phase is independently deployable. Phase 1 is prerequisite for all others.

---

